name: contrail-metrics

image:
  repository: silviakazmi/contrail-metrics
  tag: "35.1"
  port: 8000

replicas: 1

route:
  enabled: false

svc:
  type: ClusterIP
  additionalPorts: false
  extraPorts: 
    - name: udp-514
      protocol: UDP
      port: 514
      targetPort: 514

envVars:
  enabled: false
  vars:
    - name: KAFKA_BROKERCONNECT
      value: "mano-kafka-cluster-kafka-bootstrap.kafka-cluster-operator.svc.cluster.local:9092"
    - name: KAFKA_PROPERTIES
      value: ""
    - name: KAFKA_TRUSTSTORE
      value: ""
    - name: KAFKA_KEYSTORE
      value: ""
    - name: JVM_OPTS
      value: ""
    - name: JMX_PORT
      value: "8686"
    - name: HOST
      value: ""
    - name: SERVER_SERVLET_CONTEXTPATH
      value: ""
    - name: KAFKA_PROPERTIES_FILE
      value: "kafka.properties"
    - name: KAFKA_TRUSTSTORE_FILE
      value: "kafka.truststore.jks"
    - name: KAFKA_KEYSTORE_FILE
      value: "kafka.keystore.jks"
    - name: SERVER_PORT
      value: "9000"
    - name: CMD_ARGS
      value: ""

configmap:
  enabled: false
  volumeMounts:
    - name: telegraf-conf
      mountPath: /etc/telegraf/
  volumes:
    - name: telegraf-conf
      configMap:
        name: telegraf-conf
  data: 
    telegraf.conf: |
      # Global tags can be specified here in key="value" format.
      [global_tags]
        # dc = "us-east-1" # will tag all metrics with dc=us-east-1
        # rack = "1a"
        ## Environment variables can be used as tags, and throughout the config file
        # user = "$USER"


      # Configuration for telegraf agent
      [agent]
        ## Default data collection interval for all inputs
        # interval = "10s"
        ## Rounds collection interval to 'interval'
        ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
        round_interval = true

        ## Telegraf will send metrics to outputs in batches of at
        ## most metric_batch_size metrics.
        metric_batch_size = 1000
        ## For failed writes, telegraf will cache metric_buffer_limit metrics for each
        ## output, and will flush this buffer on a successful write. Oldest metrics
        ## are dropped first when this buffer fills.
        metric_buffer_limit = 10000

        ## Collection jitter is used to jitter the collection by a random amount.
        ## Each plugin will sleep for a random time within jitter before collecting.
        ## This can be used to avoid many plugins querying things like sysfs at the
        ## same time, which can have a measurable effect on the system.
        collection_jitter = "0s"

        ## Default flushing interval for all outputs. You shouldn't set this below
        ## interval. Maximum flush_interval will be flush_interval + flush_jitter
        flush_interval = "5s"
        ## Jitter the flush interval by a random amount. This is primarily to avoid
        ## large write spikes for users running a large number of telegraf instances.
        ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
        flush_jitter = "0s"

        ## Run telegraf in debug mode
        debug = false
        ## Run telegraf in quiet mode
        quiet = false
        ## Override default hostname, if empty use os.Hostname()
        hostname = ""
        ## If set to true, do no set the "host" tag in the telegraf agent.
        omit_hostname = false

      ###############################################################################
      #                            INPUT PLUGINS                                    #
      ###############################################################################
      [[inputs.aos]]

        port = 8080

        # Address of the server running Telegraf, it's need to be reacheable from AOS
        address = "telegraf-metrics.integrations.svc.cluster.local"

        # Streaming Type Can be "perfmon", "alerts" or "events"
        streaming_type = [ "perfmon" ]

        refresh_interval = 20
        
        # Define parameter to join the AOS Server using the REST API
        aos_server = "172.31.255.54"
        aos_port = 443
        aos_login = "admin"
        aos_password = "admin"


      ###############################################################################
      #                            OUTPUT PLUGINS                                   #
      ###############################################################################

      [[outputs.kafka]]
        ## URLs of kafka brokers
        brokers = ["mano-kafka-cluster-kafka-bootstrap.kafka-cluster-operator.svc.cluster.local:9092"]
        ## Kafka topic for producer messages
        topic = "telegraf-metrics"

        ## Optional topic suffix configuration.
        ## If the section is omitted, no suffix is used.
        ## Following topic suffix methods are supported:
        ##   measurement - suffix equals to separator + measurement's name
        ##   tags        - suffix equals to separator + specified tags' values
        ##                 interleaved with separator

        ## Suffix equals to "_" + measurement's name
        # [outputs.kafka.topic_suffix]
        #   method = "measurement"
        #   separator = "_"

        ## Suffix equals to "__" + measurement's "foo" tag value.
        ##   If there's no such a tag, suffix equals to an empty string
        # [outputs.kafka.topic_suffix]
        #   method = "tags"
        #   keys = ["foo"]
        #   separator = "__"

        ## Suffix equals to "_" + measurement's "foo" and "bar"
        ##   tag values, separated by "_". If there is no such tags,
        ##   their values treated as empty strings.
        # [outputs.kafka.topic_suffix]
        #   method = "tags"
        #   keys = ["foo", "bar"]
        #   separator = "_"

        ## Telegraf tag to use as a routing key
        ##  ie, if this tag exists, its value will be used as the routing key
        routing_tag = "host"

        ## CompressionCodec represents the various compression codecs recognized by
        ## Kafka in messages.
        ##  0 : No compression
        ##  1 : Gzip compression
        ##  2 : Snappy compression
        compression_codec = 0

        ##  RequiredAcks is used in Produce Requests to tell the broker how many
        ##  replica acknowledgements it must see before responding
        ##   0 : the producer never waits for an acknowledgement from the broker.
        ##       This option provides the lowest latency but the weakest durability
        ##       guarantees (some data will be lost when a server fails).
        ##   1 : the producer gets an acknowledgement after the leader replica has
        ##       received the data. This option provides better durability as the
        ##       client waits until the server acknowledges the request as successful
        ##       (only messages that were written to the now-dead leader but not yet
        ##       replicated will be lost).
        ##   -1: the producer gets an acknowledgement after all in-sync replicas have
        ##       received the data. This option provides the best durability, we
        ##       guarantee that no messages will be lost as long as at least one in
        ##       sync replica remains.
        required_acks = -1

        ##  The total number of times to retry sending a message
        max_retry = 3

        ## Optional SSL Config
        # ssl_ca = "/etc/telegraf/ca.pem"
        # ssl_cert = "/etc/telegraf/cert.pem"
        # ssl_key = "/etc/telegraf/key.pem"
        ## Use SSL but skip chain & host verification
        # insecure_skip_verify = false

        ## Optional SASL Config
        # sasl_username = "kafka"
        # sasl_password = "secret"

        data_format = "json"
